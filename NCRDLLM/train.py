import os
import sys
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.amp import autocast, GradScaler  # ğŸ”§ ä½¿ç”¨torch.amp.GradScaler
from torch.cuda.amp import GradScaler  # ğŸ”§ ä¿®å¤: ä½¿ç”¨torch.cuda.amp.GradScaler
from tqdm import tqdm
import torch.nn.functional as F  # ğŸ†• æ·»åŠ è¿™è¡Œ
from config import config
from args_parser import parse_args
from dataset import (
    load_all_features,
    load_positive_pairs,
    prepare_cv_splits,
    RNADrugDataset
)
from model import create_model
from utils import (
    set_seed,
    create_experiment_dir,
    backup_code,
    save_experiment_info,
    calculate_metrics,
    print_metrics,
    aggregate_cv_results,
    print_cv_summary,
    EarlyStopping
)
from export_utils import (
    save_fused_features,
    save_predictions,
    save_modality_weights,
    aggregate_cv_features,
    aggregate_cv_raw_features,
    aggregate_cv_predictions,
    save_fold_results
)
from visualize import (
    plot_roc_curve,
    plot_pr_curve,
    plot_cv_roc_curves,
    plot_cv_pr_curves,
    plot_metrics_comparison,
    plot_modality_tsne_features
)

# ========== ğŸ†• æŸå¤±å‡½æ•°å®šä¹‰ ==========
class FocalLoss(nn.Module):
    """
    Focal Loss for addressing class imbalance
    
    ğŸ”§ ä¿®å¤: alphaç°åœ¨æ­£ç¡®æ”¯æŒç±»åˆ«æƒé‡
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        # alphaå¯ä»¥æ˜¯:
        # - å•ä¸ªfloat: æ­£ç±»æƒé‡, è´Ÿç±»æƒé‡è‡ªåŠ¨ä¸º1-alpha
        # - list/tuple: [è´Ÿç±»æƒé‡, æ­£ç±»æƒé‡]
        if isinstance(alpha, (float, int)):
            self.alpha = torch.tensor([1-alpha, alpha])
        else:
            self.alpha = torch.tensor(alpha)
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: [B, 2] logits
            targets: [B] labels (0 or 1)
        """
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        
        # æ ¹æ®ç›®æ ‡ç±»åˆ«é€‰æ‹©å¯¹åº”çš„alpha
        at = self.alpha.to(inputs.device)[targets]
        focal_loss = at * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class LabelSmoothingLoss(nn.Module):
    """
    Label Smoothing Loss
    
    ğŸ”§ æ”¹è¿›: ä½¿ç”¨æ›´ç¨³å®šçš„å®ç°
    """
    def __init__(self, classes=2, smoothing=0.1):
        super(LabelSmoothingLoss, self).__init__()
        self.smoothing = smoothing
        self.classes = classes
        self.confidence = 1.0 - smoothing
    
    def forward(self, pred, target):
        """
        Args:
            pred: [B, 2] logits
            target: [B] labels (0 or 1)
        """
        log_probs = F.log_softmax(pred, dim=-1)
        
        with torch.no_grad():
            # æ„å»ºå¹³æ»‘åçš„çœŸå®åˆ†å¸ƒ
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.classes - 1))
            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
        
        # è®¡ç®—KLæ•£åº¦æŸå¤±
        loss = -torch.sum(true_dist * log_probs, dim=-1)
        return loss.mean()

def create_loss_function(config, train_dataset=None):
    """
    åˆ›å»ºæŸå¤±å‡½æ•°
    
    ğŸ”§ ä¿®å¤: weighted_ceçš„æ ‡ç­¾æå–é€»è¾‘
    """
    if config.LOSS_TYPE == 'ce':
        return nn.CrossEntropyLoss()
    
    elif config.LOSS_TYPE == 'focal':
        return FocalLoss(alpha=config.FOCAL_ALPHA, gamma=config.FOCAL_GAMMA)
    
    elif config.LOSS_TYPE == 'label_smoothing':
        return LabelSmoothingLoss(classes=2, smoothing=config.LABEL_SMOOTHING)
    
    elif config.LOSS_TYPE == 'weighted_ce':
        if config.CLASS_WEIGHTS is None and train_dataset is not None:
            # ğŸ”§ ä¿®å¤: ç›´æ¥ä½¿ç”¨datasetçš„labelså±æ€§
            labels = train_dataset.labels  # âœ… æ­£ç¡®!
            class_counts = torch.bincount(torch.tensor(labels))
            class_weights = 1.0 / class_counts.float()
            class_weights = class_weights / class_weights.sum() * 2  # å½’ä¸€åŒ–
            
            print(f"   ğŸ“Š è‡ªåŠ¨è®¡ç®—ç±»åˆ«æƒé‡: è´Ÿç±»={class_weights[0]:.4f}, æ­£ç±»={class_weights[1]:.4f}")
        else:
            class_weights = torch.tensor(config.CLASS_WEIGHTS or [1.0, 1.0])
        
        return nn.CrossEntropyLoss(weight=class_weights.to(config.DEVICE))
    
    else:
        raise ValueError(f"Unknown loss type: {config.LOSS_TYPE}")


def create_optimizer(model, config):
    """
    åˆ›å»ºä¼˜åŒ–å™¨
    
    âœ… è¿™ä¸ªå‡½æ•°æ˜¯æ­£ç¡®çš„,ä¸éœ€è¦ä¿®æ”¹
    """
    if config.OPTIMIZER_TYPE == 'adamw':
        return torch.optim.AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY,
            betas=config.ADAM_BETAS,
            eps=config.ADAM_EPS
        )
    
    elif config.OPTIMIZER_TYPE == 'adam':
        return torch.optim.Adam(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY,
            betas=config.ADAM_BETAS,
            eps=config.ADAM_EPS
        )
    
    elif config.OPTIMIZER_TYPE == 'sgd':
        return torch.optim.SGD(
            model.parameters(),
            lr=config.LEARNING_RATE,
            momentum=config.SGD_MOMENTUM,
            weight_decay=config.WEIGHT_DECAY,
            nesterov=config.SGD_NESTEROV
        )
    
    elif config.OPTIMIZER_TYPE == 'rmsprop':
        return torch.optim.RMSprop(
            model.parameters(),
            lr=config.LEARNING_RATE,
            alpha=0.99,
            eps=1e-8,
            weight_decay=config.WEIGHT_DECAY
        )
    
    else:
        raise ValueError(f"Unknown optimizer type: {config.OPTIMIZER_TYPE}")

def prepare_batch_features(batch, config):
    """
    ä»batchä¸­æå–ç‰¹å¾å¹¶ç»„ç»‡æˆæ¨¡å‹è¾“å…¥æ ¼å¼
    
    Args:
        batch: dict, æ¥è‡ªDataLoader
        config: Configå¯¹è±¡
    
    Returns:
        feature_dict: dict, æ¨¡å‹forwardæ‰€éœ€çš„å‚æ•°
    """
    feature_dict = {}
    
    # RNAç‰¹å¾
    if config.USE_RNA_SEQ and 'rna_seq' in batch:
        feature_dict['rna_seq_features'] = batch['rna_seq'].to(config.DEVICE)
    
    if config.USE_RNA_STRUCT and 'rna_struct' in batch:
        feature_dict['rna_struct_features'] = batch['rna_struct'].to(config.DEVICE)
    
    if config.USE_RNA_DISEASE and 'rna_disease' in batch:
        feature_dict['rna_disease_features'] = batch['rna_disease'].to(config.DEVICE)
    
    # Drugç‰¹å¾
    if config.USE_DRUG_SEQ and 'drug_seq' in batch:
        feature_dict['drug_seq_features'] = batch['drug_seq'].to(config.DEVICE)
    
    if config.USE_DRUG_STRUCT:
        if 'drug_graph' in batch and 'drug_ecfp' in batch:
            drug_graph = batch['drug_graph'].to(config.DEVICE)
            drug_ecfp = batch['drug_ecfp'].to(config.DEVICE)
            feature_dict['drug_struct_features'] = torch.cat([drug_graph, drug_ecfp], dim=-1)
    
    if config.USE_DRUG_DISEASE and 'drug_disease' in batch:
        feature_dict['drug_disease_features'] = batch['drug_disease'].to(config.DEVICE)
    
    return feature_dict


def train_one_epoch(model, dataloader, criterion, optimizer, scaler, epoch, config):
    """è®­ç»ƒä¸€ä¸ªepoch - ä¿®å¤bfloat16å…¼å®¹æ€§"""
    model.train()
    total_loss = 0
    num_batches = len(dataloader)
    
    pbar = tqdm(enumerate(dataloader), total=num_batches, desc=f"Epoch {epoch}")
    
    for batch_idx, batch in pbar:
        feature_dict = prepare_batch_features(batch, config)
        labels = batch['label'].to(config.DEVICE)
        
        # ç¡®å®šautocastçš„dtype
        if config.USE_MIXED_PRECISION:
            if torch.cuda.is_bf16_supported():
                autocast_dtype = torch.bfloat16
            else:
                autocast_dtype = torch.float16
        else:
            autocast_dtype = torch.float32
        
        with autocast('cuda', enabled=config.USE_MIXED_PRECISION, dtype=autocast_dtype):
            logits = model(**feature_dict)
            loss = criterion(logits, labels)
            loss = loss / config.ACCUMULATION_STEPS
        
        # bfloat16ä¸éœ€è¦GradScaler
        if config.USE_MIXED_PRECISION and autocast_dtype == torch.bfloat16:
            loss.backward()  # ç›´æ¥backward
        elif config.USE_MIXED_PRECISION and autocast_dtype == torch.float16:
            scaler.scale(loss).backward()  # float16éœ€è¦scaler
        else:
            loss.backward()  # float32æ­£å¸¸backward
        
        if (batch_idx + 1) % config.ACCUMULATION_STEPS == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            # bfloat16ä¸éœ€è¦GradScaler
            if config.USE_MIXED_PRECISION and autocast_dtype == torch.float16:
                scaler.step(optimizer)
                scaler.update()
            else:
                optimizer.step()
            
            optimizer.zero_grad()
        
        total_loss += loss.item() * config.ACCUMULATION_STEPS
        pbar.set_postfix({'loss': f'{loss.item() * config.ACCUMULATION_STEPS:.4f}'})
    
    avg_loss = total_loss / num_batches
    return avg_loss

def validate_and_extract(model, dataloader, criterion, fold, save_dir, config, extract_features=True):
    """
    éªŒè¯å¹¶æå–ç‰¹å¾(ç”¨äºå¯è§†åŒ–)
    
    Args:
        model: æ¨¡å‹
        dataloader: DataLoader
        criterion: æŸå¤±å‡½æ•°
        fold: int, æŠ˜æ•°
        save_dir: str, ä¿å­˜ç›®å½•
        config: Configå¯¹è±¡
        extract_features: bool, æ˜¯å¦æå–ç‰¹å¾
    
    Returns:
        metrics: dict, è¯„ä¼°æŒ‡æ ‡
        fold_data: dict, åŒ…å«y_trueå’Œy_prob,ç”¨äºç»˜åˆ¶æ›²çº¿
        feature_data: dict, åŒ…å«å„æ¨¡æ€ç‰¹å¾å’Œæ ‡ç­¾,ç”¨äºt-SNEå¯è§†åŒ–
    """
    model.eval()
    
    all_labels = []
    all_preds = []
    all_probs = []
    all_rna_ids = []
    all_drug_ids = []
    
    # ç”¨äºå­˜å‚¨èåˆç‰¹å¾
    all_rna_fused = []
    all_drug_fused = []
    
    # ğŸ†• ç”¨äºå­˜å‚¨å„æ¨¡æ€ç‰¹å¾(ç”¨äºt-SNEå¯è§†åŒ–)
    all_modality_features = {
        'rna_seq': [],
        'rna_struct': [],
        'rna_disease': [],
        'drug_seq': [],
        'drug_struct': [],
        'drug_disease': [],
        'labels': []
    }
    
    total_loss = 0
    num_batches = len(dataloader)
    
    # ğŸ”§ ä¿®å¤: ç¡®å®šautocastçš„dtype
    if config.USE_MIXED_PRECISION:
        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
            autocast_dtype = torch.bfloat16
        else:
            autocast_dtype = torch.float16
    else:
        autocast_dtype = torch.float32
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc=f"Validating Fold {fold}"):
            feature_dict = prepare_batch_features(batch, config)
            labels = batch['label'].to(config.DEVICE)
            
            # ğŸ”§ ä¿®å¤: æ·»åŠ dtypeå‚æ•°
            with autocast('cuda', enabled=config.USE_MIXED_PRECISION, dtype=autocast_dtype):
                logits = model(**feature_dict)
                loss = criterion(logits, labels)
            
            total_loss += loss.item()
            
            probs = torch.softmax(logits, dim=1)
            preds = torch.argmax(probs, dim=1)
            
            all_labels.append(labels.cpu().numpy())
            all_preds.append(preds.cpu().numpy())
            all_probs.append(probs.cpu().float().numpy())  # ğŸ”§ æ·»åŠ .float()è½¬æ¢
            all_rna_ids.extend(batch['rna_id'])
            all_drug_ids.extend(batch['drug_id'])
            
            # ğŸ†• æå–å„æ¨¡æ€ç‰¹å¾(ç”¨äºt-SNEå¯è§†åŒ–)
            if extract_features and config.SAVE_FEATURES:
                modality_feats = model.get_modality_features(**feature_dict)
                
                if modality_feats['rna_seq'] is not None:
                    all_modality_features['rna_seq'].append(modality_feats['rna_seq'].cpu())
                if modality_feats['rna_struct'] is not None:
                    all_modality_features['rna_struct'].append(modality_feats['rna_struct'].cpu())
                if modality_feats['rna_disease'] is not None:
                    all_modality_features['rna_disease'].append(modality_feats['rna_disease'].cpu())
                if modality_feats['drug_seq'] is not None:
                    all_modality_features['drug_seq'].append(modality_feats['drug_seq'].cpu())
                if modality_feats['drug_struct'] is not None:
                    all_modality_features['drug_struct'].append(modality_feats['drug_struct'].cpu())
                if modality_feats['drug_disease'] is not None:
                    all_modality_features['drug_disease'].append(modality_feats['drug_disease'].cpu())
                
                all_modality_features['labels'].append(labels.cpu())
                
                # æå–èåˆç‰¹å¾
                if config.POOLING_METHOD == 'learnable_weight':
                    rna_fused, drug_fused = model.get_fused_features(**feature_dict)
                    all_rna_fused.append(rna_fused.cpu())
                    all_drug_fused.append(drug_fused.cpu())
    
    # æ‹¼æ¥æ‰€æœ‰ç»“æœ
    y_true = np.concatenate(all_labels)
    y_pred = np.concatenate(all_preds)
    y_prob = np.concatenate(all_probs)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(y_true, y_pred, y_prob)
    metrics['loss'] = total_loss / num_batches
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    save_predictions(all_rna_ids, all_drug_ids, y_true, y_pred, y_prob, fold, save_dir)
    
    # ä¿å­˜èåˆç‰¹å¾
    if extract_features and config.SAVE_FEATURES and config.POOLING_METHOD == 'learnable_weight' and all_rna_fused:
        rna_fused_tensor = torch.cat(all_rna_fused, dim=0)
        drug_fused_tensor = torch.cat(all_drug_fused, dim=0)
        save_fused_features(
            rna_fused_tensor, drug_fused_tensor,
            all_rna_ids, all_drug_ids,
            fold, save_dir, config
        )
    
    # ğŸ†• æ•´ç†ç‰¹å¾æ•°æ®ç”¨äºt-SNEå¯è§†åŒ–
    feature_data = None
    if extract_features and config.SAVE_FEATURES:
        feature_data = {}
        
        if all_modality_features['rna_seq']:
            feature_data['rna_seq'] = torch.cat(all_modality_features['rna_seq'], dim=0).cpu().float().numpy()
        if all_modality_features['rna_struct']:
            feature_data['rna_struct'] = torch.cat(all_modality_features['rna_struct'], dim=0).cpu().float().numpy()
        if all_modality_features['rna_disease']:
            feature_data['rna_disease'] = torch.cat(all_modality_features['rna_disease'], dim=0).cpu().float().numpy()
        if all_modality_features['drug_seq']:
            feature_data['drug_seq'] = torch.cat(all_modality_features['drug_seq'], dim=0).cpu().float().numpy()
        if all_modality_features['drug_struct']:
            feature_data['drug_struct'] = torch.cat(all_modality_features['drug_struct'], dim=0).cpu().float().numpy()
        if all_modality_features['drug_disease']:
            feature_data['drug_disease'] = torch.cat(all_modality_features['drug_disease'], dim=0).cpu().float().numpy()
        if all_rna_fused:
            feature_data['fused'] = np.concatenate([
                torch.cat(all_rna_fused, dim=0).cpu().float().numpy(),
                torch.cat(all_drug_fused, dim=0).cpu().float().numpy()
            ], axis=1)
        
        feature_data['labels'] = np.concatenate([l.numpy() for l in all_modality_features['labels']])

    
    # ç”¨äºç»˜åˆ¶ROC/PRæ›²çº¿çš„æ•°æ®
    fold_data = {
        'y_true': y_true,
        'y_prob': y_prob[:, 1],
        'fold': fold
    }
    
    return metrics, fold_data, feature_data


def train_one_fold(fold, train_dataset, val_dataset, experiment_dir, config):
    """
    è®­ç»ƒä¸€æŠ˜
    
    Args:
        fold: int, æŠ˜æ•°
        train_dataset: Dataset
        val_dataset: Dataset
        experiment_dir: str, å®éªŒç›®å½•
        config: Configå¯¹è±¡
    
    Returns:
        best_metrics: dict, æœ€ä½³éªŒè¯æŒ‡æ ‡
        best_fold_data: dict, æœ€ä½³éªŒè¯æ•°æ®(ç”¨äºç»˜åˆ¶æ›²çº¿)
        best_feature_data: dict, æœ€ä½³ç‰¹å¾æ•°æ®(ç”¨äºt-SNEå¯è§†åŒ–)
    """
    print(f"\n{'='*60}")
    print(f"ğŸ”„ è®­ç»ƒ Fold {fold}")
    print(f"{'='*60}")
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=config.NUM_WORKERS,
        pin_memory=config.PIN_MEMORY,
        prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=config.PIN_MEMORY,
        prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model()
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    # ğŸ†• ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = create_loss_function(config, train_dataset)
    optimizer = create_optimizer(model, config)
    

    # ğŸ”§ ä¿®å¤: æ··åˆç²¾åº¦è®­ç»ƒçš„GradScaler
    scaler = GradScaler('cuda', enabled=config.USE_MIXED_PRECISION) 
    

    # æ—©åœ
    early_stopping = EarlyStopping(
        patience=config.EARLY_STOP_PATIENCE,
        mode='max',
        delta=0.001
    )
    
    # è®­ç»ƒå¾ªç¯
    best_val_auc = 0
    best_metrics = None
    best_fold_data = None
    best_feature_data = None
    best_model_state = None

    for epoch in range(1, config.MAX_EPOCHS + 1):
        print(f"\n--- Epoch {epoch}/{config.MAX_EPOCHS} ---")
        
        # è®­ç»ƒ
        train_loss = train_one_epoch(
            model, train_loader, criterion, optimizer, scaler, epoch, config
        )
        print(f"Train Loss: {train_loss:.4f}")
        
        # éªŒè¯
        val_metrics, fold_data, feature_data = validate_and_extract(
            model, val_loader, criterion, fold, experiment_dir, config,
            extract_features=False  # è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æå–ç‰¹å¾
        )
        
        print(f"Val Loss: {val_metrics['loss']:.4f}, Val AUC: {val_metrics['auc_roc']:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['auc_roc'] > best_val_auc:
            best_val_auc = val_metrics['auc_roc']
            best_metrics = val_metrics
            best_fold_data = fold_data
            print(f"ğŸ’¾ è¾¾åˆ°æœ€ä½³æŒ‡æ ‡")
            if config.SAVE_MODEL:
                best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
                print(f"ğŸ’¾ è¾¾åˆ°æœ€ä½³æŒ‡æ ‡ï¼Œå·²ä¿å­˜æ¨¡å‹çŠ¶æ€")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_metrics['auc_roc']):
            print(f"â¹ï¸  æ—©åœè§¦å‘! æœ€ä½³AUC: {best_val_auc:.4f}")
            break
    
    print(f"\nâœ… Fold {fold} è®­ç»ƒå®Œæˆ! æœ€ä½³AUC: {best_val_auc:.4f}")

    # ğŸ†• åŠ è½½æœ€ä½³æ¨¡å‹å¹¶æå–ç‰¹å¾
    if best_model_state is not None:
        print(f"\nğŸ”„ åŠ è½½æœ€ä½³æ¨¡å‹å¹¶æå–ç‰¹å¾...")
        model.load_state_dict(best_model_state)
        model.eval()
        
        # æå–ç‰¹å¾
        best_metrics_final, best_fold_data_final, best_feature_data = validate_and_extract(
            model, val_loader, criterion, fold, experiment_dir, config,
            extract_features=True  # å¼ºåˆ¶æå–ç‰¹å¾
        )
        
        # æ›´æ–°ä¸ºæœ€ç»ˆçš„fold_data
        best_fold_data = best_fold_data_final
        
        # ä¿å­˜æ¨¡æ€æƒé‡
        if config.SAVE_WEIGHTS and config.POOLING_METHOD == 'learnable_weight':
            fold_dir = os.path.join(experiment_dir, f'fold_{fold}')
            save_modality_weights(model, fold_dir)
    
    return best_metrics, best_fold_data, best_feature_data


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("ğŸš€ RNA-Drug Interaction Prediction with Multi-Modal LLM")
    print("="*70)
    
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parse_args()
    print(config)
    
    # 2. è®¾ç½®éšæœºç§å­
    set_seed(config.RANDOM_SEED)
    
    # 3. åˆ›å»ºå®éªŒç›®å½•
    experiment_dir = create_experiment_dir(config.RESULTS_DIR)
    
    # 4. å¤‡ä»½ä»£ç 
    backup_code(experiment_dir)
    
    # 5. ä¿å­˜å®éªŒé…ç½®
    save_experiment_info(experiment_dir, config)
    
    # 6. åŠ è½½æ•°æ®
    print("\n" + "="*60)
    print("ğŸ“Š åŠ è½½æ•°æ®")
    print("="*60)
    
    rna_features_dict, drug_features_dict = load_all_features()
    positive_pairs, all_rna_ids, all_drug_ids = load_positive_pairs()
    
    cv_splits = prepare_cv_splits(
        positive_pairs, all_rna_ids, all_drug_ids,
        n_folds=config.N_FOLDS,
        negative_ratio=config.NEGATIVE_RATIO,
        seed=config.RANDOM_SEED,
        save_dir=config.SPLITS_DIR
    )
    
    # 7. äº¤å‰éªŒè¯è®­ç»ƒ
    print("\n" + "="*60)
    print("ğŸ”„ å¼€å§‹5æŠ˜äº¤å‰éªŒè¯")
    print("="*60)
    
    fold_results = []
    fold_data_list = []
    fold_feature_data_list = []  # ğŸ†• å­˜å‚¨æ¯æŠ˜çš„ç‰¹å¾æ•°æ®
    
    for fold in range(config.N_FOLDS):
        train_dataset = RNADrugDataset(
            cv_splits[fold]['train_pairs'],
            rna_features_dict,
            drug_features_dict
        )
        
        val_dataset = RNADrugDataset(
            cv_splits[fold]['val_pairs'],
            rna_features_dict,
            drug_features_dict
        )
        
        # è®­ç»ƒ
        best_metrics, fold_data, feature_data = train_one_fold(
            fold, train_dataset, val_dataset, experiment_dir, config
        )
        
        fold_results.append(best_metrics)
        fold_data_list.append(fold_data)
        fold_feature_data_list.append(feature_data)  # ğŸ†• ä¿å­˜ç‰¹å¾æ•°æ®
        
        # æ‰“å°å½“å‰æŠ˜ç»“æœ
        print_metrics(best_metrics, phase=f"Fold {fold} Best Validation")
    
    # 8. æ±‡æ€»ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š æ±‡æ€»äº¤å‰éªŒè¯ç»“æœ")
    print("="*60)
    
    aggregated = aggregate_cv_results(fold_results)
    print_cv_summary(aggregated)
    
    results = {
        'fold_results': fold_results,
        'aggregated': aggregated,
        'config': {
            'dataset': config.DATASET_NAME,
            'model_type': config.MODEL_TYPE,
            'pooling_method': config.POOLING_METHOD,
            'enabled_modalities': config.get_enabled_modalities(),
            'use_lora': config.USE_LORA,
            'batch_size': config.BATCH_SIZE,
            'learning_rate': config.LEARNING_RATE
        }
    }
    
    save_fold_results(results, experiment_dir)
    
    # 9. æ‹¼æ¥äº”æŠ˜ç‰¹å¾å’Œé¢„æµ‹ç»“æœ
    print("\n" + "="*60)
    print("ğŸ”— æ‹¼æ¥äº”æŠ˜æ•°æ®")
    print("="*60)
    
    if config.SAVE_FEATURES:
        aggregate_cv_features(experiment_dir, config.N_FOLDS)
        aggregate_cv_raw_features(experiment_dir, config.N_FOLDS)
    else:
        print("   âš ï¸  ç‰¹å¾ä¿å­˜å·²ç¦ç”¨ï¼Œè·³è¿‡ç‰¹å¾æ‹¼æ¥")

    aggregate_cv_predictions(experiment_dir, config.N_FOLDS)
    
    # 10. ç»˜åˆ¶äº”æŠ˜æ±‡æ€»æ›²çº¿
    print("\n" + "="*60)
    print("ğŸ“Š ç»˜åˆ¶äº”æŠ˜æ±‡æ€»æ›²çº¿")
    print("="*60)
    
    plot_cv_roc_curves(fold_data_list, os.path.join(experiment_dir, 'cv_roc_curves.png'))
    plot_cv_pr_curves(fold_data_list, os.path.join(experiment_dir, 'cv_pr_curves.png'))
    plot_metrics_comparison(fold_results, os.path.join(experiment_dir, 'metrics_comparison.png'))
    
    # ğŸ†• 11. ç»˜åˆ¶t-SNEå¯è§†åŒ–
    if config.SAVE_FEATURES and any(fd is not None for fd in fold_feature_data_list):
        print("\n" + "="*60)
        print("ğŸ¨ ç»˜åˆ¶t-SNEç‰¹å¾å¯è§†åŒ–")
        print("="*60)
        
        for fold, feature_data in enumerate(fold_feature_data_list):
            if feature_data is not None:
                fold_dir = os.path.join(experiment_dir, f'fold_{fold}')
                os.makedirs(fold_dir, exist_ok=True)
                plot_modality_tsne_features(feature_data, fold, fold_dir, config)
    
    # 12. å®Œæˆ
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {experiment_dir}")
    print("="*70)
    
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   AUC-ROC: {aggregated['auc_roc']['mean']:.4f} Â± {aggregated['auc_roc']['std']:.4f}")
    print(f"   PR-AUC:  {aggregated['pr_auc']['mean']:.4f} Â± {aggregated['pr_auc']['std']:.4f}")
    print(f"   F1:      {aggregated['f1']['mean']:.4f} Â± {aggregated['f1']['std']:.4f}")
    
    print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
    if config.SAVE_FEATURES:
        print(f"   - èåˆç‰¹å¾: all_rna_fused_features.csv, all_drug_fused_features.csv")
        print(f"   - åŸå§‹ç‰¹å¾: all_rna_raw_features.csv, all_drug_raw_features.csv")
        print(f"   - t-SNEå¯è§†åŒ–: fold_*/tsne_*.png")
    else:
        print(f"   - èåˆç‰¹å¾: æœªä¿å­˜ï¼ˆä½¿ç”¨ --no_save_features ç¦ç”¨ï¼‰")
        print(f"   - åŸå§‹ç‰¹å¾: æœªä¿å­˜ï¼ˆä½¿ç”¨ --no_save_features ç¦ç”¨ï¼‰")
    print(f"   - é¢„æµ‹ç»“æœ: predictions_simple.csv, details_predictions_simple.csv")
    print(f"   - ROC/PRæ›²çº¿: cv_roc_curves.png, cv_pr_curves.png")
    if config.SAVE_WEIGHTS:
        print(f"   - æ¨¡æ€æƒé‡: fold_*/modality_weights.json")
    else:
        print(f"   - æ¨¡æ€æƒé‡: æœªä¿å­˜ï¼ˆä½¿ç”¨ --no_save_weights ç¦ç”¨ï¼‰")
    print("\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­!")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)