import os
os.environ["TRANSFORMERS_NO_TORCHVISION"] = "1"

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType
from config import config


# ========== åŸºç¡€ç»„ä»¶ ==========

class FeatureAdapter(nn.Module):
    """
    ç‰¹å¾Adapter:å°†ä¸åŒç»´åº¦çš„ç‰¹å¾æ˜ å°„åˆ°LLM embeddingç©ºé—´
    
    ä¸‰ç§ç±»å‹:
    1. åºåˆ—ç‰¹å¾ (640D/768D â†’ 4096D)
    2. ç»“æ„ç‰¹å¾ (128D/1024D â†’ 4096D)  
    3. ç–¾ç—…ç‰¹å¾ (1690D â†’ 512D â†’ 4096D,å‹ç¼©ç¨€ç–æ€§)
    """
    def __init__(self, input_dim, output_dim=4096, adapter_type='normal'):
        super().__init__()
        self.adapter_type = adapter_type
        
        if adapter_type == 'disease':
            # ç–¾ç—…ç‰¹å¾:å…ˆå‹ç¼©ç¨€ç–æ€§
            self.adapter = nn.Sequential(
                nn.Linear(input_dim, 512),
                nn.LayerNorm(512),
                nn.GELU(),
                nn.Dropout(0.2),
                nn.Linear(512, output_dim),
                nn.LayerNorm(output_dim)
            )
        else:
            # æ™®é€šç‰¹å¾:æ¸è¿›å¼å‡ç»´
            if input_dim <= 256:
                hidden_dim = 1024
            elif input_dim <= 768:
                hidden_dim = 2048
            else:
                hidden_dim = 2048
            
            self.adapter = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(0.2),
                nn.Linear(hidden_dim, output_dim),
                nn.LayerNorm(output_dim)
            )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        return self.adapter(x)


class WeightedPooling(nn.Module):
    """å¯å­¦ä¹ æƒé‡çš„æ± åŒ–(learnable_weightæ–¹æ³•)"""
    def __init__(self, num_features):
        super().__init__()
        self.weights = nn.Parameter(torch.ones(num_features) / num_features)
    
    def forward(self, features):
        """
        Args:
            features: list of [B, D], é•¿åº¦ä¸ºnum_features
        Returns:
            pooled: [B, D]
        """
        weights = F.softmax(self.weights, dim=0)
        stacked = torch.stack(features, dim=1)  # [B, num_features, D]
        pooled = (stacked * weights.view(1, -1, 1)).sum(dim=1)  # [B, D]
        return pooled
    
    def get_normalized_weights(self):
        """è·å–å½’ä¸€åŒ–åçš„æƒé‡"""
        with torch.no_grad():
            return F.softmax(self.weights, dim=0).cpu().numpy()


class TwoLayerClassifier(nn.Module):
    """2å±‚MLPåˆ†ç±»å¤´"""
    def __init__(self, input_dim, hidden_dim=1024, dropout=0.3):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 2)
        )
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                if m.out_features == 2:
                    nn.init.normal_(m.weight, mean=0, std=0.01)
                else:
                    nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        return self.classifier(x)


# ========== ä¸»æ¨¡å‹ ==========

class MultimodalLLM(nn.Module):
    """
    å¤šæ¨¡æ€LLM - ä¿®å¤æ··åˆç²¾åº¦è®­ç»ƒç‰ˆæœ¬
    
    ğŸ”§ å…³é”®ä¿®å¤:
    1. æ”¯æŒtorch.bfloat16åŠ è½½LLM (æ›´ç¨³å®šçš„æ··åˆç²¾åº¦)
    2. ä¿®å¤GradScalerå¯¼å…¥
    3. è‡ªåŠ¨æ ¹æ®GPUèƒ½åŠ›é€‰æ‹©ç²¾åº¦
    """
    
    def __init__(self):
        super().__init__()
        
        print("\n" + "="*60)
        print("ğŸ”§ åˆå§‹åŒ–å¤šæ¨¡æ€LLM (æ··åˆç²¾åº¦ä¼˜åŒ–ç‰ˆ)")
        print("="*60)
        
        # 1. åŠ è½½LLM
        self._load_llm()
        
        # 2. æ·»åŠ ç‰¹æ®Štokens
        self._add_special_tokens()
        
        # 3. åˆ›å»ºAdapters(æ ¹æ®å¯ç”¨çš„æ¨¡æ€)
        self._create_adapters()
        
        # 4. åˆ›å»ºåˆ†ç±»å¤´
        self._create_classifier()
        
        self._print_model_info()
        print("="*60 + "\n")
    
    def _load_llm(self):
        """
        åŠ è½½LLMå¹¶é…ç½®LoRA
        
        ğŸ”§ å…³é”®ä¿®å¤:
        - ä½¿ç”¨torch.bfloat16è€Œä¸æ˜¯torch.float32
        - bfloat16åœ¨Ampere GPU (A100/RTX 30xx+)ä¸Šæ›´ç¨³å®š
        - å¦‚æœä¸æ”¯æŒbfloat16,è‡ªåŠ¨é™çº§åˆ°float16
        """
        print("   ğŸ“¥ åŠ è½½LLMæ¨¡å‹...")
        
        # possible_paths = [
        #     "./llama3.1/LLM-Research/Meta-Llama-3___1-8B-Instruct",
        #     "./llama3.1/LLM-Research/Meta-Llama-3.1-8B-Instruct",
            
        #     config.LLM_MODEL_ID,
        # ]
        possible_paths = [
            "./llama3.1/LLM-Research/Llama-3___2-3B-Instruct",
            "./llama3.1/LLM-Research/Meta-Llama-3.1-3B-Instruct",
            
            config.LLM_MODEL_ID,
        ]
        # possible_paths = [
        #     "./llama3.1/LLM-Research/Llama-3___2-1B-Instruct",
            
        #     config.LLM_MODEL_ID,
        # ]
        
        model_path = None
        for path in possible_paths:
            if os.path.exists(path):
                model_path = path
                print(f"   âœ… æ‰¾åˆ°æœ¬åœ°æ¨¡å‹: {path}")
                break
        
        if model_path is None:
            raise RuntimeError("âŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹")
        
        # åŠ è½½Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="left",
            local_files_only=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ğŸ”§ æ ¹æ®æ··åˆç²¾åº¦é…ç½®é€‰æ‹©åŠ è½½ç²¾åº¦
        if config.USE_MIXED_PRECISION:
            # æ£€æŸ¥GPUæ˜¯å¦æ”¯æŒbfloat16
            if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
                load_dtype = torch.bfloat16
                print(f"   âœ… ä½¿ç”¨ bfloat16 åŠ è½½LLM (æ›´ç¨³å®šçš„æ··åˆç²¾åº¦)")
            else:
                load_dtype = torch.float16
                print(f"   âœ… ä½¿ç”¨ float16 åŠ è½½LLM (æ ‡å‡†æ··åˆç²¾åº¦)")
        else:
            load_dtype = torch.float32
            print(f"   â„¹ï¸  ä½¿ç”¨ float32 åŠ è½½LLM (å…¨ç²¾åº¦è®­ç»ƒ)")
        
        # åŠ è½½æ¨¡å‹
        self.llm = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map=None,
            torch_dtype=load_dtype,  # ğŸ”§ ä¿®å¤: æ ¹æ®é…ç½®åŠ¨æ€é€‰æ‹©ç²¾åº¦
            trust_remote_code=True,
            local_files_only=True
        ).to(config.DEVICE)
                
        # é…ç½®LoRA
        if config.USE_LORA:
            print(f"   ğŸ”§ é…ç½®LoRA (r={config.LORA_R}, alpha={config.LORA_ALPHA})...")
            
            lora_config = LoraConfig(
                r=config.LORA_R,
                lora_alpha=config.LORA_ALPHA,
                target_modules=config.LORA_TARGET_MODULES,
                lora_dropout=config.LORA_DROPOUT,
                bias="none",
                task_type=TaskType.CAUSAL_LM
            )
            
            self.llm = get_peft_model(self.llm, lora_config)
            
            trainable = sum(p.numel() for p in self.llm.parameters() if p.requires_grad)
            total = sum(p.numel() for p in self.llm.parameters())
            print(f"   âœ… LoRAå·²å¯ç”¨:")
            print(f"      å¯è®­ç»ƒå‚æ•°: {trainable:,} ({100*trainable/total:.2f}%)")
            print(f"      ç›®æ ‡æ¨¡å—: {len(config.LORA_TARGET_MODULES)}ä¸ª")
        else:
            for param in self.llm.parameters():
                param.requires_grad = False
            print("   ğŸ”’ LLMå‚æ•°å·²å†»ç»“")
        
        self.llm_dtype = self.llm.dtype
        print(f"   ğŸ“Š LLMè¿è¡Œç²¾åº¦: {self.llm_dtype}")
    
    def _add_special_tokens(self):
        """æ·»åŠ ç‰¹æ®Štokens"""
        special_tokens = {
            'additional_special_tokens': [
                '<RNA_SEQ>', '<RNA_STRUCT>', '<RNA_DISEASE>',
                '<DRUG_SEQ>', '<DRUG_STRUCT>', '<DRUG_DISEASE>',
                '<CLS>'
            ]
        }
        num_added = self.tokenizer.add_special_tokens(special_tokens)
        self.llm.resize_token_embeddings(len(self.tokenizer))
        
        # ä¿å­˜token IDs
        self.rna_seq_token_id = self.tokenizer.convert_tokens_to_ids('<RNA_SEQ>')
        self.rna_struct_token_id = self.tokenizer.convert_tokens_to_ids('<RNA_STRUCT>')
        self.rna_disease_token_id = self.tokenizer.convert_tokens_to_ids('<RNA_DISEASE>')
        self.drug_seq_token_id = self.tokenizer.convert_tokens_to_ids('<DRUG_SEQ>')
        self.drug_struct_token_id = self.tokenizer.convert_tokens_to_ids('<DRUG_STRUCT>')
        self.drug_disease_token_id = self.tokenizer.convert_tokens_to_ids('<DRUG_DISEASE>')
        self.cls_token_id = self.tokenizer.convert_tokens_to_ids('<CLS>')
        
        print(f"   âœ… æ·»åŠ äº†{num_added}ä¸ªç‰¹æ®Štoken")
    
    def _create_adapters(self):
        """åˆ›å»ºAdapters"""
        print(f"   ğŸ”§ åˆ›å»ºAdapters (ç›®æ ‡ç»´åº¦: {config.LLM_HIDDEN_DIM}D)...")
        
        # RNA Adapters
        if config.USE_RNA_SEQ:
            self.rna_seq_adapter = FeatureAdapter(config.RNA_SEQ_DIM, config.LLM_HIDDEN_DIM)
            print(f"      âœ… RNAåºåˆ—Adapter: {config.RNA_SEQ_DIM}D â†’ {config.LLM_HIDDEN_DIM}D")
        
        if config.USE_RNA_STRUCT:
            self.rna_struct_adapter = FeatureAdapter(config.RNA_STRUCT_DIM, config.LLM_HIDDEN_DIM)
            print(f"      âœ… RNAç»“æ„Adapter: {config.RNA_STRUCT_DIM}D â†’ {config.LLM_HIDDEN_DIM}D")
        
        if config.USE_RNA_DISEASE:
            self.rna_disease_adapter = FeatureAdapter(
                config.RNA_DISEASE_DIM, config.LLM_HIDDEN_DIM, adapter_type='disease'
            )
            print(f"      âœ… RNAç–¾ç—…Adapter: {config.RNA_DISEASE_DIM}D â†’ {config.LLM_HIDDEN_DIM}D (å‹ç¼©)")
        
        # Drug Adapters
        if config.USE_DRUG_SEQ:
            self.drug_seq_adapter = FeatureAdapter(config.DRUG_SEQ_DIM, config.LLM_HIDDEN_DIM)
            print(f"      âœ… Drugåºåˆ—Adapter: {config.DRUG_SEQ_DIM}D â†’ {config.LLM_HIDDEN_DIM}D")
        
        if config.USE_DRUG_STRUCT:
            self.drug_struct_adapter = FeatureAdapter(config.DRUG_STRUCT_DIM, config.LLM_HIDDEN_DIM)
            print(f"      âœ… Drugç»“æ„Adapter: {config.DRUG_STRUCT_DIM}D â†’ {config.LLM_HIDDEN_DIM}D")
        
        if config.USE_DRUG_DISEASE:
            self.drug_disease_adapter = FeatureAdapter(
                config.DRUG_DISEASE_DIM, config.LLM_HIDDEN_DIM, adapter_type='disease'
            )
            print(f"      âœ… Drugç–¾ç—…Adapter: {config.DRUG_DISEASE_DIM}D â†’ {config.LLM_HIDDEN_DIM}D (å‹ç¼©)")
    
    def _create_classifier(self):
        """åˆ›å»ºåˆ†ç±»å¤´"""
        print(f"   ğŸ”§ åˆ›å»ºåˆ†ç±»å¤´: {config.POOLING_METHOD.upper()}...")
        
        # åˆ›å»ºPoolingå±‚(å¦‚æœéœ€è¦)
        if config.POOLING_METHOD == 'learnable_weight':
            rna_modalities = sum([
                config.USE_RNA_SEQ,
                config.USE_RNA_STRUCT,
                config.USE_RNA_DISEASE
            ])
            drug_modalities = sum([
                config.USE_DRUG_SEQ,
                config.USE_DRUG_STRUCT,
                config.USE_DRUG_DISEASE
            ])
            
            if rna_modalities > 1:
                self.rna_pooling = WeightedPooling(rna_modalities)
                print(f"      âœ… RNAåŠ æƒæ± åŒ–: {rna_modalities}ä¸ªæ¨¡æ€")
            
            if drug_modalities > 1:
                self.drug_pooling = WeightedPooling(drug_modalities)
                print(f"      âœ… DrugåŠ æƒæ± åŒ–: {drug_modalities}ä¸ªæ¨¡æ€")
            
            classifier_input_dim = config.LLM_HIDDEN_DIM * 2
        
        elif config.POOLING_METHOD == 'cls':
            classifier_input_dim = config.LLM_HIDDEN_DIM
        
        else:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„åˆ†ç±»å¤´æ–¹æ¡ˆ: {config.POOLING_METHOD}")
        
        # åˆ›å»ºåˆ†ç±»å™¨
        self.classifier = TwoLayerClassifier(
            input_dim=classifier_input_dim,
            hidden_dim=config.CLASSIFIER_HIDDEN_DIM,
            dropout=config.CLASSIFIER_DROPOUT
        )
        print(f"      âœ… åˆ†ç±»å™¨: {classifier_input_dim}D â†’ {config.CLASSIFIER_HIDDEN_DIM}D â†’ 2")
    
    def _process_through_llm(self, embeds_dict, batch_size, device):
        """
        é€šè¿‡LLMå¤„ç†embeddings
        
        Args:
            embeds_dict: {token_id: [B, D]}
            batch_size: int
            device: torch.device
        
        Returns:
            processed: {token_id: [B, D]}
        """
        # æ„å»ºinput_idså’Œembeddings
        token_ids = list(embeds_dict.keys())
        seq_len = len(token_ids)
        
        input_ids = torch.tensor([token_ids] * batch_size, dtype=torch.long, device=device)
        
        # è·å–embeddings
        embeds = self.llm.get_input_embeddings()
        
        # æ›¿æ¢ç‰¹æ®Štokençš„embeddings
        inputs_embeds = embeds(input_ids)
        for idx, token_id in enumerate(token_ids):
            inputs_embeds[:, idx, :] = embeds_dict[token_id]
        
        # é€šè¿‡LLM
        outputs = self.llm(
            inputs_embeds=inputs_embeds,
            output_hidden_states=True,
            return_dict=True
        )
        
        # æå–æœ€åä¸€å±‚çš„hidden states
        last_hidden = outputs.hidden_states[-1]
        
        # æ„å»ºè¾“å‡ºå­—å…¸
        processed = {}
        for idx, token_id in enumerate(token_ids):
            processed[token_id] = last_hidden[:, idx, :]
        
        return processed
    
    def get_modality_features(self, rna_seq_features=None, drug_seq_features=None,
                              rna_struct_features=None, drug_struct_features=None,
                              rna_disease_features=None, drug_disease_features=None):
        """
        è·å–å„æ¨¡æ€çš„LLMå¤„ç†åç‰¹å¾(ç”¨äºt-SNEå¯è§†åŒ–)
        
        Returns:
            dict: {
                'rna_seq': [B, 4096] or None,
                'drug_seq': [B, 4096] or None,
                ...
            }
        """
        # è·å–batch_sizeå’Œdevice
        batch_size, device = None, None
        for feat in [rna_seq_features, drug_seq_features, rna_struct_features,
                     drug_struct_features, rna_disease_features, drug_disease_features]:
            if feat is not None:
                batch_size = feat.shape[0]
                device = feat.device
                break
        
        # Step 1: å‡†å¤‡embeddings
        embeds_dict = {}
        
        if config.USE_RNA_SEQ and rna_seq_features is not None:
            embeds_dict[self.rna_seq_token_id] = self.rna_seq_adapter(rna_seq_features)
        if config.USE_RNA_STRUCT and rna_struct_features is not None:
            embeds_dict[self.rna_struct_token_id] = self.rna_struct_adapter(rna_struct_features)
        if config.USE_RNA_DISEASE and rna_disease_features is not None:
            embeds_dict[self.rna_disease_token_id] = self.rna_disease_adapter(rna_disease_features)
        
        if config.USE_DRUG_SEQ and drug_seq_features is not None:
            embeds_dict[self.drug_seq_token_id] = self.drug_seq_adapter(drug_seq_features)
        if config.USE_DRUG_STRUCT and drug_struct_features is not None:
            embeds_dict[self.drug_struct_token_id] = self.drug_struct_adapter(drug_struct_features)
        if config.USE_DRUG_DISEASE and drug_disease_features is not None:
            embeds_dict[self.drug_disease_token_id] = self.drug_disease_adapter(drug_disease_features)
        
        # Step 2: é€šè¿‡LLMå¤„ç†
        processed = self._process_through_llm(embeds_dict, batch_size, device)
        
        # Step 3: è¿”å›å„æ¨¡æ€ç‰¹å¾
        return {
            'rna_seq': processed.get(self.rna_seq_token_id),
            'rna_struct': processed.get(self.rna_struct_token_id),
            'rna_disease': processed.get(self.rna_disease_token_id),
            'drug_seq': processed.get(self.drug_seq_token_id),
            'drug_struct': processed.get(self.drug_struct_token_id),
            'drug_disease': processed.get(self.drug_disease_token_id)
        }
    
    def get_fused_features(self, rna_seq_features=None, drug_seq_features=None,
                          rna_struct_features=None, drug_struct_features=None,
                          rna_disease_features=None, drug_disease_features=None):
        """
        è·å–èåˆåçš„RNAå’ŒDrugç‰¹å¾
        
        Returns:
            rna_fused: [B, 4096]
            drug_fused: [B, 4096]
        """
        # è·å–å„æ¨¡æ€ç‰¹å¾
        modality_feats = self.get_modality_features(
            rna_seq_features, drug_seq_features,
            rna_struct_features, drug_struct_features,
            rna_disease_features, drug_disease_features
        )
        
        # RNAç‰¹å¾æ± åŒ–
        rna_features = []
        if modality_feats['rna_seq'] is not None:
            rna_features.append(modality_feats['rna_seq'])
        if modality_feats['rna_struct'] is not None:
            rna_features.append(modality_feats['rna_struct'])
        if modality_feats['rna_disease'] is not None:
            rna_features.append(modality_feats['rna_disease'])
        
        if len(rna_features) == 1:
            rna_fused = rna_features[0]
        else:
            rna_fused = self.rna_pooling(rna_features)
        
        # Drugç‰¹å¾æ± åŒ–
        drug_features = []
        if modality_feats['drug_seq'] is not None:
            drug_features.append(modality_feats['drug_seq'])
        if modality_feats['drug_struct'] is not None:
            drug_features.append(modality_feats['drug_struct'])
        if modality_feats['drug_disease'] is not None:
            drug_features.append(modality_feats['drug_disease'])
        
        if len(drug_features) == 1:
            drug_fused = drug_features[0]
        else:
            drug_fused = self.drug_pooling(drug_features)
        
        return rna_fused, drug_fused
    
    def forward(self, rna_seq_features=None, drug_seq_features=None,
                rna_struct_features=None, drug_struct_features=None,
                rna_disease_features=None, drug_disease_features=None):
        """
        å‰å‘ä¼ æ’­
        
        Returns:
            logits: [B, 2]
        """
        # è·å–batch_sizeå’Œdevice
        batch_size, device = None, None
        for feat in [rna_seq_features, drug_seq_features, rna_struct_features,
                     drug_struct_features, rna_disease_features, drug_disease_features]:
            if feat is not None:
                batch_size = feat.shape[0]
                device = feat.device
                break
        
        # Step 1: å‡†å¤‡embeddings
        embeds_dict = {}
        
        if config.USE_RNA_SEQ and rna_seq_features is not None:
            embeds_dict[self.rna_seq_token_id] = self.rna_seq_adapter(rna_seq_features)
        if config.USE_RNA_STRUCT and rna_struct_features is not None:
            embeds_dict[self.rna_struct_token_id] = self.rna_struct_adapter(rna_struct_features)
        if config.USE_RNA_DISEASE and rna_disease_features is not None:
            embeds_dict[self.rna_disease_token_id] = self.rna_disease_adapter(rna_disease_features)
        
        if config.USE_DRUG_SEQ and drug_seq_features is not None:
            embeds_dict[self.drug_seq_token_id] = self.drug_seq_adapter(drug_seq_features)
        if config.USE_DRUG_STRUCT and drug_struct_features is not None:
            embeds_dict[self.drug_struct_token_id] = self.drug_struct_adapter(drug_struct_features)
        if config.USE_DRUG_DISEASE and drug_disease_features is not None:
            embeds_dict[self.drug_disease_token_id] = self.drug_disease_adapter(drug_disease_features)
        
        # Step 2: æ·»åŠ CLS Tokençš„å ä½embedding
        if config.POOLING_METHOD == 'cls':
            embeds_dict[self.cls_token_id] = torch.zeros(
                batch_size, config.LLM_HIDDEN_DIM, device=device
            )
        
        # Step 3: é€šè¿‡LLMå¤„ç†
        processed = self._process_through_llm(embeds_dict, batch_size, device)
        
        # Step 4: æ ¹æ®åˆ†ç±»å¤´æ–¹æ¡ˆæå–ç‰¹å¾
        if config.POOLING_METHOD == 'cls':
            cls_feat = processed[self.cls_token_id]
            logits = self.classifier(cls_feat)
        
        else:  # 'learnable_weight' or 'attention'
            rna_features = []
            if config.USE_RNA_SEQ and self.rna_seq_token_id in processed:
                rna_features.append(processed[self.rna_seq_token_id])
            if config.USE_RNA_STRUCT and self.rna_struct_token_id in processed:
                rna_features.append(processed[self.rna_struct_token_id])
            if config.USE_RNA_DISEASE and self.rna_disease_token_id in processed:
                rna_features.append(processed[self.rna_disease_token_id])
            
            drug_features = []
            if config.USE_DRUG_SEQ and self.drug_seq_token_id in processed:
                drug_features.append(processed[self.drug_seq_token_id])
            if config.USE_DRUG_STRUCT and self.drug_struct_token_id in processed:
                drug_features.append(processed[self.drug_struct_token_id])
            if config.USE_DRUG_DISEASE and self.drug_disease_token_id in processed:
                drug_features.append(processed[self.drug_disease_token_id])
            
            # æ± åŒ–
            if len(rna_features) == 1:
                rna_pooled = rna_features[0]
            else:
                rna_pooled = self.rna_pooling(rna_features)
            
            if len(drug_features) == 1:
                drug_pooled = drug_features[0]
            else:
                drug_pooled = self.drug_pooling(drug_features)
            
            # æ‹¼æ¥å¹¶åˆ†ç±»
            combined = torch.cat([rna_pooled, drug_pooled], dim=-1)
            logits = self.classifier(combined)
        
        return logits
    
    def predict_proba(self, **kwargs):
        """é¢„æµ‹æ¦‚ç‡"""
        logits = self.forward(**kwargs)
        return torch.softmax(logits, dim=1)
    
    def predict(self, **kwargs):
        """é¢„æµ‹ç±»åˆ«"""
        probs = self.predict_proba(**kwargs)
        return torch.argmax(probs, dim=1)
    
    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f"\n   ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"      æ€»å‚æ•°é‡: {total_params / 1e6:.1f}M")
        print(f"      å¯è®­ç»ƒå‚æ•°é‡: {trainable_params / 1e6:.1f}M")
        print(f"      å†»ç»“å‚æ•°é‡: {(total_params - trainable_params) / 1e6:.1f}M")
        print(f"      å¯è®­ç»ƒæ¯”ä¾‹: {100*trainable_params/total_params:.2f}%")
        
        if config.USE_LORA:
            llm_trainable = sum(p.numel() for p in self.llm.parameters() if p.requires_grad)
            print(f"\n      LLMå¯è®­ç»ƒå‚æ•°: {llm_trainable / 1e6:.2f}M")
        
        adapter_params = 0
        for name, module in self.named_modules():
            if 'adapter' in name.lower():
                adapter_params += sum(p.numel() for p in module.parameters())
        print(f"      Adapterå‚æ•°: {adapter_params / 1e6:.2f}M")
        
        classifier_params = sum(p.numel() for p in self.classifier.parameters())
        print(f"      åˆ†ç±»å¤´å‚æ•°: {classifier_params / 1e6:.2f}M")
        
    def get_modality_weights(self):
        """
        è·å–æ‰€æœ‰æ¨¡æ€çš„æƒé‡
        
        Returns:
            dict: {'rna_weights': [...], 'drug_weights': [...]}
        """
        weights_dict = {}
        
        if hasattr(self, 'rna_pooling') and isinstance(self.rna_pooling, WeightedPooling):
            weights_dict['rna_weights'] = self.rna_pooling.get_normalized_weights().tolist()
        
        if hasattr(self, 'drug_pooling') and isinstance(self.drug_pooling, WeightedPooling):
            weights_dict['drug_weights'] = self.drug_pooling.get_normalized_weights().tolist()
        
        return weights_dict


# ========== åˆ›å»ºæ¨¡å‹çš„å·¥å‚å‡½æ•° ==========
def create_model():
    """æ ¹æ®config.MODEL_TYPEåˆ›å»ºæ¨¡å‹"""
    print("\n" + "="*60)
    print(f"ğŸ”§ åˆ›å»ºæ¨¡å‹: {config.MODEL_TYPE.upper()}")
    print("="*60)
    
    if config.MODEL_TYPE == 'llm':
        model = MultimodalLLM()
    elif config.MODEL_TYPE == 'baseline':
        from baseline import BaselineMLP
        version = getattr(config, 'BASELINE_VERSION', 'strong')
        model = BaselineMLP(version=version)
    else:
        raise ValueError(f"âŒ æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {config.MODEL_TYPE}")
    
    model = model.to(config.DEVICE)
    
    print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {config.DEVICE}\n")
    
    return model