import os
import json
import pickle
import random
import shutil
from datetime import datetime
import numpy as np
import torch
from sklearn.metrics import (
    accuracy_score, 
    roc_auc_score, 
    f1_score, 
    precision_score, 
    recall_score,
    average_precision_score,
    confusion_matrix
)


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­,ç¡®ä¿å¯å¤çŽ°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def get_timestamp():
    """èŽ·å–å½“å‰æ—¶é—´æˆ³å­—ç¬¦ä¸²"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def create_experiment_dir(base_dir="./results"):
    """
    åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å®žéªŒç›®å½•
    
    Returns:
        experiment_dir: å®žéªŒç›®å½•è·¯å¾„,ä¾‹å¦‚ ./results/exp_20250116_143522/
    """
    timestamp = get_timestamp()
    experiment_dir = os.path.join(base_dir, f"exp_{timestamp}")
    os.makedirs(experiment_dir, exist_ok=True)
    
    print(f"\nðŸ“ å®žéªŒç›®å½•å·²åˆ›å»º: {experiment_dir}")
    return experiment_dir


def backup_code(experiment_dir):
    """
    å¤‡ä»½å½“å‰è¿è¡Œçš„ä»£ç åˆ°å®žéªŒç›®å½•
    
    Args:
        experiment_dir: å®žéªŒç›®å½•è·¯å¾„
    """
    code_backup_dir = os.path.join(experiment_dir, 'code_snapshot')
    os.makedirs(code_backup_dir, exist_ok=True)
    
    # éœ€è¦å¤‡ä»½çš„æ–‡ä»¶åˆ—è¡¨
    files_to_backup = [
        'train.py',
        'model.py',
        'baseline.py',  # ðŸ†• æ–°å¢ž
        'dataset.py',
        'config.py',
        'utils.py',
        'visualize.py',
    ]
    
    print(f"\nðŸ’¾ å¤‡ä»½ä»£ç åˆ°: {code_backup_dir}")
    
    for filename in files_to_backup:
        if os.path.exists(filename):
            shutil.copy2(filename, os.path.join(code_backup_dir, filename))
            print(f"   âœ… {filename}")
        else:
            print(f"   âš ï¸  {filename} ä¸å­˜åœ¨,è·³è¿‡")
    
    print(f"âœ… ä»£ç å¤‡ä»½å®Œæˆ\n")


def save_experiment_info(experiment_dir, config):
    """
    ä¿å­˜å®žéªŒé…ç½®å’ŒçŽ¯å¢ƒä¿¡æ¯
    
    Args:
        experiment_dir: å®žéªŒç›®å½•è·¯å¾„
        config: é…ç½®å¯¹è±¡
    """
    info = {
        'timestamp': get_timestamp(),
        'experiment_dir': experiment_dir,
        
        # é…ç½®ä¿¡æ¯
        'config': {
            'model_type': config.MODEL_TYPE,
            'n_folds': config.N_FOLDS,
            'negative_ratio': config.NEGATIVE_RATIO,
            'batch_size': config.BATCH_SIZE,
            'accumulation_steps': config.ACCUMULATION_STEPS,  # ðŸ†•
            'effective_batch_size': config.BATCH_SIZE * config.ACCUMULATION_STEPS,  # ðŸ†•
            'learning_rate': config.LEARNING_RATE,
            'weight_decay': config.WEIGHT_DECAY,
            'max_epochs': config.MAX_EPOCHS,
            'early_stop_patience': config.EARLY_STOP_PATIENCE,
            'random_seed': config.RANDOM_SEED,
            'mixed_precision': config.USE_MIXED_PRECISION,  # ðŸ†•
        },
        
        # çŽ¯å¢ƒä¿¡æ¯
        'environment': {
            'pytorch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,
            'device': str(config.DEVICE),
            'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
        },
        
        # ðŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å˜é‡å
        'data_paths': {
            'rna_seq_feature': config.RNA_SEQ_FEATURE_PATH,  # âœ… ä¿®å¤
            'drug_seq_feature': config.DRUG_SEQ_FEATURE_PATH,  # âœ… ä¿®å¤
            'rna_struct_feature': config.RNA_STRUCT_FEATURE_PATH,
            'drug_graph_feature': config.DRUG_GRAPH_FEATURE_PATH,  # âœ… ä¿®å¤
            'drug_ecfp_feature': config.DRUG_ECFP_FEATURE_PATH,  # âœ… ä¿®å¤
            'rna_disease_feature': config.RNA_DISEASE_FEATURE_PATH,  # ðŸ†•
            'drug_disease_feature': config.DRUG_DISEASE_FEATURE_PATH,  # ðŸ†•
            'positive_pairs': config.POSITIVE_PAIRS_PATH,
        },
        
        # ðŸ†• æ¨¡æ€ä¿¡æ¯
        'modalities': {
            'enabled': config.get_enabled_modalities(),
            'rna_seq': config.USE_RNA_SEQ,
            'rna_struct': config.USE_RNA_STRUCT,
            'rna_disease': config.USE_RNA_DISEASE,
            'drug_seq': config.USE_DRUG_SEQ,
            'drug_struct': config.USE_DRUG_STRUCT,
            'drug_disease': config.USE_DRUG_DISEASE,
        }
    }
    
    # æ ¹æ®æ¨¡åž‹ç±»åž‹æ·»åŠ ç‰¹å®šé…ç½®
    if config.MODEL_TYPE == 'llm':
        info['config'].update({
            'llm_model_id': config.LLM_MODEL_ID,
            'llm_hidden_dim': config.LLM_HIDDEN_DIM,  # ðŸ†•
            'use_lora': config.USE_LORA,
            'lora_r': config.LORA_R if config.USE_LORA else None,
            'lora_alpha': config.LORA_ALPHA if config.USE_LORA else None,
            'lora_target_modules': config.LORA_TARGET_MODULES if config.USE_LORA else None,
            'pooling_method': config.POOLING_METHOD,
            'classifier_hidden_dim': config.CLASSIFIER_HIDDEN_DIM,  # ðŸ†•
        })
    elif config.MODEL_TYPE == 'baseline':
        info['config'].update({
            'total_input_dim': config.get_total_input_dim(),
        })
    
    # ä¿å­˜ä¸ºJSON
    info_path = os.path.join(experiment_dir, 'experiment_info.json')
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=4, ensure_ascii=False)
    
    print(f"ðŸ“ å®žéªŒä¿¡æ¯å·²ä¿å­˜: {info_path}\n")


def calculate_metrics(y_true, y_pred, y_prob):
    """
    è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
    
    Args:
        y_true: çœŸå®žæ ‡ç­¾ (numpy array)
        y_pred: é¢„æµ‹æ ‡ç­¾ (numpy array)
        y_prob: é¢„æµ‹æ¦‚çŽ‡ (numpy array, shape: [N, 2])
    
    Returns:
        dict: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'auc_roc': roc_auc_score(y_true, y_prob[:, 1]),
        'pr_auc': average_precision_score(y_true, y_prob[:, 1]),
        'f1': f1_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
    }
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    metrics['confusion_matrix'] = {
        'TN': int(tn), 'FP': int(fp),
        'FN': int(fn), 'TP': int(tp)
    }
    
    return metrics


def print_metrics(metrics, phase="Validation"):
    """æ‰“å°æŒ‡æ ‡"""
    print(f"\n{'='*50}")
    print(f"{phase} Metrics:")
    print(f"{'='*50}")
    print(f"Accuracy:  {metrics['accuracy']:.4f}")
    print(f"AUC-ROC:   {metrics['auc_roc']:.4f}")
    print(f"PR-AUC:    {metrics['pr_auc']:.4f}")
    print(f"F1 Score:  {metrics['f1']:.4f}")
    print(f"Precision: {metrics['precision']:.4f}")
    print(f"Recall:    {metrics['recall']:.4f}")
    print(f"\nConfusion Matrix:")
    cm = metrics['confusion_matrix']
    print(f"  TN: {cm['TN']:4d}  |  FP: {cm['FP']:4d}")
    print(f"  FN: {cm['FN']:4d}  |  TP: {cm['TP']:4d}")
    print(f"{'='*50}\n")


def save_results(results, save_path):
    """ä¿å­˜ç»“æžœåˆ°JSONæ–‡ä»¶"""
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=4, ensure_ascii=False)


def load_results(load_path):
    """ä»ŽJSONæ–‡ä»¶åŠ è½½ç»“æžœ"""
    with open(load_path, 'r', encoding='utf-8') as f:
        results = json.load(f)
    return results


def save_pickle(data, path):
    """
    ä¿å­˜æ•°æ®ä¸ºpickleæ ¼å¼
    
    Args:
        data: è¦ä¿å­˜çš„æ•°æ®
        path: ä¿å­˜è·¯å¾„
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'wb') as f:
        pickle.dump(data, f)
    print(f"   ðŸ’¾ å·²ä¿å­˜: {path}")


def load_pickle(path):
    """
    ä»Žpickleæ–‡ä»¶åŠ è½½æ•°æ®
    
    Args:
        path: pickleæ–‡ä»¶è·¯å¾„
    
    Returns:
        åŠ è½½çš„æ•°æ®
    """
    with open(path, 'rb') as f:
        data = pickle.load(f)
    return data


def aggregate_cv_results(fold_results):
    """
    æ±‡æ€»äº¤å‰éªŒè¯ç»“æžœ
    
    Args:
        fold_results: list of dict, æ¯æŠ˜çš„ç»“æžœ
    
    Returns:
        dict: å¹³å‡å€¼å’Œæ ‡å‡†å·®
    """
    metrics_names = ['accuracy', 'auc_roc', 'pr_auc', 'f1', 'precision', 'recall']
    aggregated = {}
    
    for metric in metrics_names:
        values = [fold[metric] for fold in fold_results]
        aggregated[metric] = {
            'mean': float(np.mean(values)),
            'std': float(np.std(values)),
            'values': values
        }
    
    return aggregated


def print_cv_summary(aggregated_results):
    """æ‰“å°äº¤å‰éªŒè¯æ±‡æ€»ç»“æžœ"""
    print("\n" + "="*60)
    print("ðŸŽ¯ 5-Fold Cross-Validation Summary")
    print("="*60)
    
    metrics_order = ['accuracy', 'auc_roc', 'pr_auc', 'f1', 'precision', 'recall']
    metrics_display = {
        'accuracy': 'Accuracy',
        'auc_roc': 'AUC-ROC',
        'pr_auc': 'PR-AUC',
        'f1': 'F1 Score',
        'precision': 'Precision',
        'recall': 'Recall'
    }
    
    for metric in metrics_order:
        if metric in aggregated_results:
            mean = aggregated_results[metric]['mean']
            std = aggregated_results[metric]['std']
            print(f"{metrics_display[metric]:12s}: {mean:.4f} Â± {std:.4f}")
    
    print("="*60 + "\n")


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=10, mode='max', delta=0):
        """
        Args:
            patience: å®¹å¿çš„è½®æ•°
            mode: 'max' æˆ– 'min'
            delta: æœ€å°å˜åŒ–é˜ˆå€¼
        """
        self.patience = patience
        self.mode = mode
        self.delta = delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
            return False
        
        if self.mode == 'max':
            if score > self.best_score + self.delta:
                self.best_score = score
                self.counter = 0
            else:
                self.counter += 1
        else:  # mode == 'min'
            if score < self.best_score - self.delta:
                self.best_score = score
                self.counter = 0
            else:
                self.counter += 1
        
        if self.counter >= self.patience:
            self.early_stop = True
            return True
        
        return False